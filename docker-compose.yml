version: "3.9"

services:
  # ollama:
  #     image: ollama/ollama
  #     ports:
  #       - "11434:11434"
  #     tty: true
  llama:
    image: llamastack/distribution-ollama
    container_name: llama-stack
    restart: always
    ports:
      - "8321:8321"
    volumes:
      - ~/.llama:/root/.llama
    environment:
      - INFERENCE_MODEL=llama3.2:3b
      - LLAMA_STACK_PORT=8321
      - OLLAMA_URL=http://host.docker.internal:11434
    depends_on:
      - backend
  
  backend:
    build:
      context: .
      dockerfile: Dockerfile.backend
    container_name: llamathon-backend
    volumes:
      - ./backend:/app
    env_file:
      - .env
    ports:
      - "8000:8000"
    depends_on:
      - db

  db:
    image: postgres:15
    container_name: llamathon-db
    restart: always
    env_file:
      - .env
    volumes:
      - pgdata:/var/lib/postgresql/data
    ports:
      - "5432:5432"

  frontend:
    build:
      context: .
      dockerfile: Dockerfile.frontend
    container_name: llamathon-frontend
    ports:
      - "5173:5173"
    volumes:
      - ./frontend:/app
      - frontend_node_modules:/app/node_modules

volumes:
  pgdata:
  frontend_node_modules:
